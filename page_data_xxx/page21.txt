图模型
===

1 贝叶斯网络
---
---
**1** 虽然可以通过代数运算表示、操作概率模型，但有时使用图来描述概率分布是十分方便并且有益的，这类模型也被称为概率图模型。

**2** 贝叶斯网络也称有向图模型，其中结点表示一个或一组随机变量，有向边表示变量之间的概率关系。马尔科夫随机场则又称无向图模型，与贝叶斯网络不同，模型中的无向边表示随机变量的软约束关系而非因果关系。

**3** 假设贝叶斯网络中有 $K$ 个结点，则随机变量联合分布为

$$
p(\mathbf{x})=\prod_{k=1}^Kp(x_k|\text{pa}_k),
$$
其中 $\text{pa}_k$ 表示结点 $x_k$ 的父结点， $\mathbf{x}=\{x_1,...,x_K\}$ 。

**4** 我们讨论的有向图一般是无环的，也即有向无环图(DAG)。特别地，通过重新安排结点编号，可使父结点的编号值始终小于子结点。

**5** 重新考虑多项式回归问题，其中随机变量包括多项式系数 $\mathbf{w}$ 以及观测数据 $\mathbf{t}$ ，
>观测数据应该是随机变量，因为我们假设观测值是有噪的，第一次的观测值和第二次未必相同，它服从某个概率分布，我们得到是观测值（样本）而不是分布本身。

此外模型还包括输入数据 $\mathbf{x}=\left(x_1,...,x_N\right)^\text{T}$ 以及噪声方差 $\sigma^2$ 、系数先验的精度 $\alpha$ 。随机变量（ $\mathbf{w}$ 和 $\mathbf{t}$ ）的联合分布为

$$
p(\mathbf{t,w})=p(\mathbf{w})\prod_{n=1}^Np(t_n|\mathbf{w}),
$$

其图表示如下
![o_Untitled Diagram](https://www.pandox.xyz/api/db/images/pandoxone/o_Untitled%20Diagram%20(1).png "80")
为了使表示更加紧凑，将 $N$ 个同类型的节点合并并增加一个方框。将除随机变量外的其余数据增加到图结构中，由于


$$
p(\mathbf{t,w}|\mathbf{x},\alpha,\sigma^2)=p(\mathbf{w}|\alpha)\prod_{n=1}^Np(t_n|\mathbf{w},x_n,\sigma^2),
$$
得到如下网络结构
<span id = "anchor1"></span>
![o_Untitled Diagram](https://www.pandox.xyz/api/db/images/pandoxone/o_Untitled%20Diagram%20(2).png "80")
在图模型中，将随机变量分为两类，一类是**观测变量(observed)**,一类是**隐含变量(latent)**(又称隐藏变量)，在本例中， $\mathbf{t}$ 属于前者，而 $\mathbf{w}$ 属于后者,在图形表示中，将观测变量涂上阴影。
在预测阶段，给定新的输入变量 $\hat{x}$ ，模型给出输出变量 $\hat{t}$ 的概率分布，满足

$$
p(\hat{t}|\hat{x},\mathbf{x,t},\alpha,\sigma^2)\propto \int p(\hat{t},\mathbf{t,w}|\hat{x},\mathbf{x},\alpha,\sigma^2) \rm  d\mathbf{w},
$$
其网络结构如下
![o_Untitled Diagram](https://www.pandox.xyz/api/db/images/pandoxone/o_Untitled%20Diagram%20(3).png "80")

**6** 考虑离散随机变量 $\mathbf{x}=\left(x_1,...,x_K\right)^\text{T}$ 有 $K$ 种不同状态 $\left(x_i\in\{0,1\},P(x_i=1)=\mu_i,\sum_{i}x_i=1\right)$ ，从而其概率分布为

$$
p(\mathbf{x}|\boldsymbol{\mu})=\prod_{k=1}^K{\mu_k^{x_k}},
$$
这里有 $K-1$ 个自由参数。同时考虑 $M$ 个离散随机变量 $\mathbf{x}_1,...,\mathbf{x}_M$ ，其中每个随机变量具有 $K$ 种状态，则自由参数为 $K^M-1$ 个，这对应表示联合分布的有向图为全联通的情况(任意低编号结点和高编号结点之间存在一条有向边)。当有向图不存在边时，自由参数为 $M(K-1)$ 。

**7** 有两种减少模型独立参数的方法，其一是参数( $\boldsymbol\mu$ )共享，此外可以使用参数化方法（而不是真值表）描述条件概率分布。假设所有结点具有两种状态 $1$ 或 $0$ ，结点 $y$ 的前驱结点为 $x_1,...,x_M$ ，从而若使用真值表确定概率分布的方法，计算

$$
p(y=1|x_1,...,x_M)
$$
需要 $2^M$ 个自由参数，从而

$$
p(y=1|x_1,...,x_M)=\prod_{i=1}^{2^M}\mu_i^{[x_1,...,x_M]_i},
$$
而若引入参数 $\mathbf{w}=\left(w_0,w_1,...,w_M\right)^\text{T},$ 可使用下式

$$
p(y=1|x_1,...,x_M)=\sigma\left(\mathbf{w}^\text{T}\mathbf{x}\right)
$$
确定分布形式，这里只用到了 $M+1$ 个自由参数。

**8** 我们也可以使用有向无环图表示多元高斯分布，即所谓**线性高斯模型**，其应用包括**概率主成分分析(probabilistic PCA)**、**因子分析**、 **线性动态系统**等。在有向图中，结点服从高斯分布，均值为其父节点取值的线性组合，方差为 $v_i$ ，从而

$$
p(x_i|\text{pa}_i)=\mathcal{N}\left(x_i| \sum_{j\in\text{pa}_i}w_{ij}x_j+b_i,v_i\right),
$$
于是联合分布对数为

$$
\begin{aligned}\ln p(\mathbf{x})&=\sum_{i=1}^D\ln p(x_i|\text{pa}_i)\\&=-\sum_{i=1}^D\frac{1}{2v_i}\left(x_i-\sum_{j\in\text{pa}_i}{w_{ij}x_j}-b_i\right)^2+\text{const},\end{aligned}
$$
其中 $\mathbf{x}=\left(x_1,...,x_D\right)^\text{T}$ ，由于取对数的结果是 $\mathbf{x}$ 的二次函数，因而联合分布是多元高斯分布。
当 $i$ 结点父结点状态给定时，该结点上随机变量 $x_i$ 满足下式

$$
x_i=\sum_{j\in\text{pa}_i}w_{ij}x_j+b_i+\sqrt{v_i}\epsilon_i,
$$
其中 $\epsilon_i\sim\mathcal{N}(0,1)$ ，且 $\mathbb{E}(\epsilon_i\epsilon_j)=I_{ij}$ ，从而 $x_i$ 期望为

$$
\mathbb{E}[x_i]=\sum_{j\in\text{pa}_i}{w_{ij}\mathbb{E}[x_j]}+b_i,
$$
协方差计算如下

$$
\begin{aligned}\text{cov}[x_i,x_j]&=\mathbb{E}[(x_i-\mathbb{E}[x_i])(x_j-\mathbb{E}[x_j])]\\&=\mathbb{E}\left[ (x_i-\mathbb{E}[x_i])\left(\sum_{k\in\text{pa}_j}w_{jk}(x_k-\mathbb{E}[x_k])+\sqrt{v_j}\epsilon_j\right)\right]\\&=\sum_{k\in\text{pa}_j}w_{jk}\text{cov}[x_i,x_k]+\mathbb{E}\left[\sqrt{v_iv_j}\epsilon_i\epsilon_j\right]\\&=\sum_{k\in\text{pa}_j}w_{jk}\text{cov}[x_i,x_k]+I_{ij}v_j.\end{aligned}
$$

2 条件独立性
---
---
**1** 考虑三个随机变量 $a$ ， $b$ 和 $c$ ，如果满足 $p(a,b|c)=p(a|c)p(b|c)$ ，则称 $a$ 、 $b$ 条件独立（给定 $c$ ）。一般来说，给定一个由多个条件概率分布相乘得到的联合分布函数，可以反复利用加法、乘法原理检验变量之间的条件独立性，除此之外，我们还可以使用**d-分离**(d表示有向)的方法，利用图的结构判定条件独立性。

**2** 考虑只有3个随机变量 $a,b,c$ 构成的图，
![o_Untitled Diagram](https://www.pandox.xyz/api/db/images/pandoxone/o_Untitled%20Diagram%20(4).png "80")
其联合分布函数为

$$
p(a,b,c)=p(a|c)p(b|c)p(c),
$$
可以说明， $a$ 、 $b$ 两变量不独立(积掉 $c$ 后)，而关于 $c$ 条件独立，并称 $c$ 结点为尾到尾型(tail-to-tail)。再考虑如下图结构，
![o_Untitled Diagram](https://www.pandox.xyz/api/db/images/pandoxone/o_Untitled%20Diagram%20(6).png "80")
其联合分布函数为

$$
p(a,b,c)=p(a)p(c|a)p(b|c),
$$
同样， $a$ 、 $b$ 仅关于 $c$ 条件独立，此时 $c$ 结点为头到尾型(head-to-tail)。而如下的图结构
![o_Untitled Diagram](https://www.pandox.xyz/api/db/images/pandoxone/o_Untitled%20Diagram%20(5).png "80")
表示的联合分布为

$$
p(a,b,c)=p(a)p(b)p(c|a,b),
$$
此时 $a$ 、 $b$ 相互独立，但在给定 $c$ 的情况下不独立，此处 $c$ 为头到头型(head-to-head)。实际上 $c$ 的任意后代给定， $a$ 、 $b$ 都将呈现相关性。

**2** 首先给出d-分离的规范表述，考虑任一个有向图， $A$ 、 $B$ 、 $C$ 是三个不相交的图上结点集合，那么给定 $C$ ,
>-  $A$ 和 $B$ 条件独立当且仅当所有从 $A$ 中任意结点到 $B$ 中任意结点的路径均被 $C$ 阻隔；
>- 路径被 $C$ 阻隔当且仅当路径上存在一个 $c$ 类结点；
>- 若路径上结点 $p\in C$ ，且 $p$ 是头到尾型或尾到尾型，则 $p$ 是 $c$ 类结点；
>- 若路径上结点 $p$ 是头到头型，且 $p$ 及其任意后代均不在 $C$ 中，则 $p$ 是 $c$ 类结点。

在[似然分布有向图](#anchor1)中， $\alpha$ 及 $\sigma^2$ 参数作为观测结点没有前驱，任意经过这些结点的路径必然被阻隔（观测结点必然是尾到尾型）。因此将这些结点从DAG中移除不影响条件独立性的判定。
> 需要注意的是，我们之前考虑从概率分布出发构建有向图，事实上，图结构本身蕴含了变量之间的独立性信息（独立或条件独立），独立性信息由图中缺失的边（相对全联通图）呈现，而条件独立性信息则依赖于观测变量（结点）的指定。可以将图作为一个过滤器（filter)看待，只能通过那些满足图上独立条件的分布函数，将这些函数构成的集合记为 $\mathcal{DF}$ ，显然随着独立条件加强， $\mathcal{DF}$ 的规模将降低。

3 马尔科夫随机场
---
---
**1** 马尔科夫场又称马尔科夫网络或无向图模型，假设 $A$ 、 $B$ 、 $C$ 是图上的三个不相交结点集合，给定 $C$ ， $A$ 和 $B$ 条件独立当且仅当 $C$ 是原图的一个割集使得 $A$ 、 $B$ 分属不同的联通分量。相对而言，无向图模型的重点在因子分解上，即给定一张无向图，每个结点代表一个随机变量，该无向图如何将联合分布 $p(\mathbf{x})$ 分解为若干因子的乘积，也就是无向图本身表达了何种条件独立特性。这种分解是重要的，因为任意两点均有边相连的图并不携带任何（独立性）信息。在因子分解的过程中依据的一个原则是：**不邻接的两点不同属一个因子**。换句话说，一个因子包含的点必然两两邻接，构成一个团。在此基础上，为了计算、表示的效率和简单，施加一个更强的条件，即要求因子中的点集作为团必须是极大团。

**2** 用 $C$ 表示一个团， $\mathbf{x}_C$ 表示团中的变量，联合分布可以写成势函数(potential function)的乘积：

$$
p(\mathbf{x})=\frac{1}{Z}\prod_C \psi_C(\mathbf{x}_C),
$$
其中 $Z$ 是作为归一化常数的配分函数(partition function)，满足

$$
Z=\sum_\mathbf{x}\prod_C\psi_C(\mathbf{x}_C).
$$
与有向图分布的因子不同，势函数本身没有直接的概率意义解释。

**3** 设 $\mathcal{UI}$ 表示满足给定无向图蕴含的条件独立性的分布集合， $\mathcal{UF}$ 表示满足可按极大团分解的分布集合，哈默斯利-克利福德说明 $\mathcal{UI}$ 和 $\mathcal{UF}$ 相等。假设势函数 $\psi_C(\mathbf{x}_C)>0$ ，将其写成：

$$
\psi_C(\mathbf{x}_C)=\exp\{-E(\mathbf{x}_C)\},
$$
其中 $E(\mathbf{x}_C)$ 称为能量函数，该指数表示也称玻尔兹曼分布。联合分布的总能量是各极大团能量之和。

**4** 图像降噪是可应用无向图模型的一个例子。假设图像是二值的，观测到的像素值为 $y_i\in\{-1,+1\},\text{ }i\in\{1,...,D\}$ ，无噪原图为 $x_i$ ： $1\leq i\leq D$ 。此外假设随机噪声以某个较小的概率翻转原图单个像素点的取值。由于噪声强度较小，像素点 $x_i$ 与 $y_i$ 之间呈现较强的相关性（表示似然概率）；此外自然图像的连续性说明原图中邻接的两点 $x_i$ 与 $x_j$ 以较高的概率取值相同（表示先验概率）。在能量函数中分别以 $-\eta x_iy_i$ 和 $-\beta x_ix_j$ 表示此前后两项。此外，对于原图中的每个像素点，在能量函数中增添 $hx_i$ 项以表示模型偏置（偏好两个像素取值中的其中一个）（**?**），从而模型的能量函数为

$$
E(\mathbf{x},\mathbf{y})=h\sum_{i}x_i-\beta\sum_{\{i,j\}}x_ix_j-\eta \sum_i x_iy_i,
$$
联合分布为

$$
p(\mathbf{x},\mathbf{y})=\frac{1}{Z}\exp\{-E(\mathbf{x},\mathbf{y})\}.
$$
给定 $\mathbf{y}$ 已观测到，可以直接计算条件概率 $p(\mathbf{x}|\mathbf{y}),$ 为此可用ICM(Iterated Conditional modes)迭代方法得到一个局部最优解，其主要过程如下：
> 1. 初始化 $\{x_i\}$ :  $x_i\leftarrow y_i;$ 
> 2. 给定 $\{x_i\}$ 的一个排列，对元素按其出现顺序重新编号， $j \leftarrow 1;$ 
> 3. 若执行 $x_j \leftarrow -x_j$ 后， $p(\mathbf{x},\mathbf{y})$ 未增大， $x_j\leftarrow -x_j$ ， $j\leftarrow j + 1$ ;
> 4. 若 $j = D+1$ ，且终止条件被触发，结束，否则执行(2)；若 $j < D+1$ ，执行(3)。

**最大乘积算法(max-product)** 可以作为对ICM的改进，尽管它也不产生全局最优解。对于某些类别的模型，基于无向图的割的算法可以给出全局最优解。

**5** 考虑由某个有向图确定的概率分布向无向图表示的转化。一般来说，我们会将若干条件概率乘积项映射到无向图模型中的某个势函数，从而归一化常数 $Z=1$ 。需要注意的是，将超过一个条件概率项合并到某个势函数中即意味着丢弃无向图模型中的部分条件独立性，以下给出的转换方法通过增添尽可能少的边从而保留尽可能多的条件独立性质。
>对于有向图中的每个结点，将其父结点集合中的任意两点用无向边连接，再用无向边替换图中的剩余有向边，得到道德图(moral graph)。此外，对于每一个包含条件概率 $p_i(·)$ 项中结点集合的（极大）团 $C$ ，执行 $p_C(\mathbf{x}_C)\leftarrow p_C(\mathbf{x}_C)p_i(·)$ （ $p_C(\mathbf{x}_C)$ 初始化为 $1$ ）。

该转换过程在联合树算法(junction tree)中有重要作用。

**6** 如果某个图反映了某个分布的所有条件独立性质，称该图是分布的 $D$  *map*（相关性）如果某个分布满足某个图中的所有条件独立性质，则称图是分布式 $I$  *map*（独立性）。显然无边图（只存在结点）是任何分布的 $D$  *map*，而全连接图（任意两点有边相连）是任何分布的 $I$  *map*。对于某个分布，如果某个图同时是该分布的 $D$  *map*和 $I$  *map*，则该图称为该分布的完美图。一个有向完美图未必有对应的（保持所有条件独立性质）无向完美图，反之亦然。链图是一种同时包含有向边和无向边的图，尽管其表达能力更强，但仍存在某些概率分布，它们的完美链图不存在。


4 图模型中的推断
---
---
**1** 图模型中的推断即固定某些结点为已观测值，计算某个剩余结点子集的后验分布。首先考虑无向图中的链（马尔科夫链），假设不存在已观测结点，计算某个结点变量的边缘分布。由于联合分布为

$$
p(\mathbf{x})=\frac{1}{Z}\psi_{1,2}(x_1,x_2)\psi_{2,3}(x_2,x_3)\cdots\psi_{N-1,N}(x_{N-1},x_N),
$$
假设结点均为离散随机变量，有 $K$ 中状态， $x_n$ 的边缘分布为

$$
p(x_n)=\sum_{x_1}\cdots\sum_{x_{n-1}}\sum_{x_{n+1}}\cdots\sum_{x_N}p(\mathbf{x}),
$$
利用条件独立性，重写上式

$$
p(x_n)=\frac{1}{Z}\mu_\alpha(x_n)\mu_\beta(x_n),
$$
>这里 $\mu_\alpha\mu_\beta$ 表示两个（ $K$ 维）列向量的Hardmard乘积(对应元素相乘)。

其中

$$
\begin{aligned}\mu_\alpha(x_n)&=\left[\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1,n})\cdots\left[\sum_{x_2}\psi_{2,3}(x_2,x_3)\left[\sum_{x_1}\psi_{1,2}(x_1,x_2)\right]\right]\right]\\\mu_\beta(x_n)&=\left[\sum_{x_{n+1}}\psi_{n,n+1}(x_n,x_{n+1})\cdots\left[\sum_{x_N}\psi_{N-1,N}(x_{N-1},x_N)\right]\right],
\end{aligned}
$$
容易看出，相对于无条件独立性的情形，计算复杂度从 $O(K^N)$ 降到了 $O(NK^2)$ 。递归规则为

$$
\begin{aligned}\mu_\alpha(x_n)&=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1},x_n)\mu_\alpha(x_{n-1}),\\\mu_\beta(x_n)&=\sum_{x_{n+1}}\psi_{n+1,n}(x_{n+1},x_n)\mu_\beta(x_{n+1}).
\end{aligned}
$$
若存在已观测结点，其联合分布计算方法类似，直接将观测值带入公式，同时移除某些求和符号。

**2** 使用**和积算法(sum-product algorithm)**，可以在树形结构上完成同样有效率的推断。无向树指任意两个结点有且仅有一条路径的无向图；而有向树指除根结点没有父节点外，其余结点恰有一个父结点的有向图。
>显然，有向树和无向树之间不需要增减任何边即可完成转换。此外，在不考虑边的方向的前提下，任意两个结点恰存在一条路径的有向图的非有向树称为polytree，如下图所示。
>![o_Untitled Diagram](https://www.pandox.xyz/api/db/images/pandoxone/o_Untitled%20Diagram%20(2)%20(1).png "80")

**3** 有向或无向图将联合分布表达为若干定义在结点（变量）子集上的因子（函数）的乘积。**因子图**则包含两类结点，其一是变量，此外还有因子函数本身。将联合分布重写为如下形式

$$
p(\mathbf{x})=\prod_sf_s(\mathbf{x}_s),
$$
其中 $\mathbf{x}_s$ 变量结点变量的子集，  $f_s(\cdot)$ 为因子函数。
因子图必然是二分图，即变量结点与因子结点之间没有边相连。使用某种规则可将无向树或有向树转换为同为树结构的因子图。如果有向图是polytree，也可将其转换为树形因子图。

**4** 假定原图是有向树或无向树或polytree中的一种，从而对应的因子图为树。进一步假设所有结点均为隐含离散变量，对于结点 $x$ ，其边缘分布为

$$
p(x)=\sum_{\mathbf{x}\backslash x}p(\mathbf{x}),
$$
考虑将 $x$ 作为树的根结点，由于图是二分的，因此 $x$ 的所有子结点均为因子函数结点，联合分布可记为如下形式

$$
p(\mathbf{x})=\prod_{s\in\text{ne}(x)}F_s(x,X_s),
$$
其中 $\text{ne}(x)$ 表示 $x$ 的邻接（儿子）结点集合， $X_s$ 表示 $f_s$ 子树中变量结点集合， $F_s(x, X_s)$ 表示 $x$ 的 $f_s$ 子树中所有因子的乘积，示意图如下。
![o_Untitled Diagram](https://www.pandox.xyz/api/db/images/pandoxone/o_Untitled%20Diagram%20(2)%20(2).png "80")
利用各因子之间的独立性，通过变换乘法和加法次序，可将边缘分布重写

$$
\begin{aligned}p(x)&=\sum_{\mathbf{x}\backslash x}p(\mathbf{x})\\&=\sum_{\cup X_s:s\in\text{ne}(x)}\prod_{s}F_s(x,X_s)\\&=\prod_{s\in\text{ne}(x)}\left[\sum_{X_s}F_s(x,X_s)\right]\\&=\prod_{s\in\text{ne}(x)}\mu_{f_s\rightarrow x}(x),
\end{aligned}
$$
把 $\mu_{f_s\rightarrow x}(x)$ 看做从因子结点 $f_s$ 传送给变量结点 $x$ 的消息(message)函数，从而 $x$ 的边缘分布等于其所有因子子节点传送给根结点的消息函数之积。由于

$$
F_s(x,X_s)=f_s(x,x_1,...,x_M)G_1(x_1,X_{s_1})...G_M(x_M,X_{s_{M}}),
$$
这里 $x_1,...,x_M$ 均为 $f_s$ 的子变量结点， $X_{s_{m}}$ 表示结点 $x_m$ 为根的子树中变量结点的集合， $G_m(\cdot)$ 是与 $F_s(\cdot)$ 同型的函数。于是消息函数为

$$
\begin{aligned}
\mu_{f_s\rightarrow x}(x)&=\sum_{X_s}F_s(x,X_s)\\&=
\sum_{x_m\in\text{ne}(f_s)\backslash x}\sum_{\cup X_{s_m}}F_s(x,X_s)\\&=
\sum_{x_m\in\text{ne}(f_s)\backslash x}f_s(x_1,...,x_M)\left[\sum_{\cup X_{s_m}}\left[\prod_{x_m\in\text{ne}(f_s)\backslash x}G_m(x_m,X_{s_m})\right]\right]\\&=
\sum_{x_m\in\text{ne}(f_s)\backslash x}f_s(x,x_1,...,x_M)\left[\prod_{x_m\in\text{ne}(f_s)\backslash x}\left[\sum_{X_{s_m}}G_m(x_m,X_{s_m})\right]\right]\\&=
\sum_{x_m\in\text{ne}(f_s)\backslash x}f_s(x,x_1,...,x_M)\left[\prod_{x_m\in\text{ne}(f_s)\backslash x}\mu_{x_m\rightarrow f_s}(x_m)\right],
\end{aligned}
$$
这里 $\mu_{x_m\rightarrow f_s}$ 是从变量结点 $x_m$ 传送给因子结点 $f_s$ 的消息函数。下面考虑 $G_m(x_m,X_{s_m})$ 的计算，很显然应该有

$$
G_m(x_m,X_{s_m})=\prod_{l\in\text{ne}(x_m)\backslash f_s}F_l(x_m,X_{m_l}),
$$
从而消息函数满足

$$
\begin{aligned}\mu_{x_m\rightarrow f_s}(x_m)&=\sum_{X_{s_m}}G_m(x_m,X_{s_m})\\&=
\prod_{l\in\text{ne}(x_m)\backslash f_s}\mu_{f_l\rightarrow x_m}(x_m),\end{aligned}
$$
于是我们建立的递归关系，从而边缘分布的计算只与消息函数的计算有关。
如果需要计算整棵树上所有结点的边缘分布，只需要注意到消息函数是有方向的，在得到某个结点的边缘分布同时，由该根节点直至叶结点反方向计算消息函数，从而得到计算任意结点所需所有方向的消息函数，将其相乘即是其边缘分布。其计算代价是计算单个结点的两倍。
对于与某个因子结点 $f_s$ 关联的所有变量结点 $\mathbf{x}_s$ ，其联合分布由下式给出

$$
p(\mathbf{x}_s)=f_s(\mathbf{x}_s)\prod_{i\in\text{ne}(f_s)}\mu_{x_i\rightarrow f_s}(x_i),
$$
只需要先将与 $f_s$ 邻接的边删除再重新考虑即可。
现在考虑存在已观测变量的情形，将 $\mathbf{x}$ 分为隐含变量 $\mathbf{u}$ 和已观测变量 $\mathbf{v}$ 两部分，观测值为 $\mathbf{\hat v}$ ，从而联合分布为

$$
p(\mathbf{u},\mathbf{v}=\mathbf{\hat{v}})=p(\mathbf{x})\prod_{i}I(v_i=\hat{v}_i).
$$