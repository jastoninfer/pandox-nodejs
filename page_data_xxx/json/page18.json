{"title":"Kernel Methods","content":"\n核方法\n===\n---\n\n1 对偶表示\n---\n---\n**1** 我们从线性参数化模型推导演绎给出核函数的形式。考虑带L2正则项（参数平方误差项）的线性回归模型，误差函数由下式给出\n\n$$\n J(\\mathbf w)=\\frac{1}{2}\\sum\\left(\\mathbf w^\\text T\\boldsymbol\\phi(\\mathbf x_n)-t_n\\right)^2+\\frac{\\lambda}{2}\\mathbf w^\\text T\\mathbf w, \n$$\n该误差函数对 $\\mathbf w$ 的梯度为\n\n$$\n\\nabla_\\mathbf w J(\\mathbf w)=\\lambda\\mathbf w+\\sum \\boldsymbol\\phi(\\mathbf x_n)\\left(\\mathbf w^\\text T\\boldsymbol\\phi(\\mathbf x_n)-t_n\\right),\n$$\n令 $\\nabla_\\mathbf w=0$ ，得\n\n$$\n\\mathbf w=\\sum a_n\\boldsymbol\\phi(\\mathbf x_n)=\\mathbf\\Phi^\\text T\\mathbf a,\n$$\n其中\n\n$$\n\\mathbf\\Phi^\\text T=(\\boldsymbol\\phi(\\mathbf x_1),...,\\boldsymbol\\phi(\\mathbf x_n)),\n$$\n\n$$\na_n=-\\frac 1\\lambda\\left(\\mathbf w^\\text T\\boldsymbol\\phi(\\mathbf x_n)-t_n)\\right),\n$$\n现在将误差项表示为 $\\mathbf a$ 的函数，将 $\\mathbf w=\\mathbf\\Phi^\\text T\\mathbf a$ 回带误差函数，得到\n\n$$\n\\begin{aligned}\nJ(\\mathbf a)=&\\frac 1 2\\sum \\left(\\mathbf a^\\text T\\mathbf\\Phi\\boldsymbol\\phi(\\mathbf x_n)-t_n\\right)^2+\\frac\\lambda 2\\mathbf a^\\text T\\mathbf\\Phi\\mathbf\\Phi^\\text T\\mathbf a\\\\=&\n\\frac\\lambda 2\\mathbf a^\\text T\\mathbf\\Phi\\mathbf\\Phi^\\text T\\mathbf a+\\frac 1 2(\\mathbf a^\\text T\\mathbf\\Phi\\mathbf\\Phi^\\text T-\\mathbf t^\\text T)(\\mathbf\\Phi\\mathbf\\Phi^\\text T\\mathbf a-\\mathbf t)\\\\=&\n\\frac\\lambda 2\\mathbf a^\\text T\\mathbf\\Phi\\mathbf\\Phi^\\text T\\mathbf a+\\frac 1 2\\left(\\mathbf a^\\text T\\mathbf\\Phi\\mathbf\\Phi^\\text T\\mathbf\\Phi\\mathbf\\Phi^\\text T\\mathbf a-\\mathbf a^\\text T\\mathbf\\Phi\\mathbf\\Phi^\\text T\\mathbf t-\\mathbf t^\\text T\\mathbf\\Phi\\mathbf\\Phi^\\text T\\mathbf a+\\mathbf t^\\text T\\mathbf t\\right),\n\\end{aligned}\n$$\n现在记格莱姆/Gram矩阵为 $\\mathbf K=\\mathbf\\Phi\\mathbf\\Phi^\\text T$ ，从而 $K_{ij}=\\boldsymbol\\phi(\\mathbf x_i)^\\text T\\boldsymbol\\phi(\\mathbf x_j)=k(\\mathbf x_i,\\mathbf x_j)=K_{ji}$ ，重写误差函数为\n\n$$\nJ(\\mathbf a)=\\frac\\lambda 2\\mathbf a^\\text T\\mathbf K\\mathbf a+\\frac 1 2\\left(\\mathbf a^\\text T\\mathbf K\\mathbf K\\mathbf a-2\\mathbf a^\\text T\\mathbf K\\mathbf t+\\mathbf t^\\text T\\mathbf t\\right),\n$$\n令其关于 $\\mathbf a$ 的梯度为零\n\n$$\n\\nabla_\\mathbf aJ(\\mathbf a)=\\lambda\\mathbf K\\mathbf a+\\mathbf K\\mathbf K\\mathbf a-\\mathbf K\\mathbf t=0,\n$$\n得到\n\n$$\n\\mathbf a=\\left(\\mathbf K+\\lambda\\mathbf I\\right)^{-1}\\mathbf t,\n$$\n由于模型输出\n\n$$\ny(\\mathbf w)=\\mathbf w^\\text T\\boldsymbol\\phi(\\mathbf x),\n$$\n用 $\\mathbf a$ 替换 $\\mathbf w$ 并利用上述结论，有\n\n$$\n\\begin{aligned}\ny(\\mathbf a)=&\\mathbf a^\\text T\\mathbf\\Phi\\boldsymbol\\phi(\\mathbf x)\\\\=&\n\\boldsymbol\\phi(\\mathbf x)^\\text T\\mathbf\\Phi^\\text T\\mathbf a\\\\=&\n\\boldsymbol\\phi(\\mathbf x)^\\text T\\left(\\boldsymbol\\phi(\\mathbf x_1),...,\\boldsymbol\\phi(\\mathbf x_n)\\right)\\left(\\mathbf K+\\lambda\\mathbf I\\right)^{-1}\\mathbf t\\\\=&\n\\mathbf k(\\mathbf x)^\\text T\\left(\\mathbf K+\\lambda\\mathbf I\\right)^{-1}\\mathbf t.\n\\end{aligned}\n$$\n\n需要注意的是，为了求 $\\mathbf a$，我们需要计算一个 $N\\times N$ 矩阵的逆， $N$ 是样本个数；而如果直接求 $\\mathbf w$ ,需要在特征空间中对一个 $M\\times M$ 的矩阵求逆，这里 $M$ 是特征空间维数；一般来说 $M\\ll N$ ,因此看起来计算量反而增大了，但是新的模型输入完全依赖于核函数 $k(\\mathbf x, \\mathbf x^\\prime)$ (除输入数据本身外)，从而可以跳过特征向量 $\\boldsymbol\\phi(\\mathbf x)$ ，不受特征维数的制约。\n\n2 构造核函数\n---\n---\n\n**1** 核函数有两种构造方法，其一是使用特征函数 $\\mathbf\\phi(\\cdot)$ 函数间接表示核函数 $k(\\cdot)$ ，这里 $k(\\mathbf x,\\mathbf x^\\prime)=\\boldsymbol\\phi(\\mathbf x)^\\text T\\boldsymbol\\phi(\\mathbf x^\\prime)$ ；另一种途径是直接构造核函数，但是核函数的选取不是随意的，它需要隐式决定某个特征函数。例如考虑核函数\n\n$$\nk(\\mathbf x,\\mathbf z)=\\left(\\mathbf x^\\text T\\mathbf z\\right)^2,\n$$\n简便起见，考虑输入空间维数为2的情况，即 $\\mathbf x = (x_1,x_2)^\\text T$ ，从而\n\n$$\n\\begin{aligned}\nk(\\mathbf x, \\mathbf z) =&\\left(\\mathbf x^\\text T\\mathbf z\\right)^2\\\\=&\n\\left(x_1z_1+x_2z_2\\right)^2\\\\=&\nx_1^2z_1^2+x_2^2z_2^2+2x_1x_2z_1z_2\\\\=&\nx_1^2\\cdot z_1^2+x_2^2\\cdot z_2^2+\\sqrt 2x_1x_2\\cdot \\sqrt 2z_1z_2\\\\=&\n\\boldsymbol\\phi(x)^\\text T\\boldsymbol\\phi(z),\n\\end{aligned}\n$$\n其中 $\\boldsymbol\\phi(\\mathbf x)=\\left(x_1^2, x_2^2,\\sqrt 2x_2x_2\\right)^\\text T$ ，显然特征向量包含原输入向量的所有二次项。\n> 当两个输入向量相等时，核函数表示该向量在特征空间中的规范二次型。\n> 当特征向量是规范的（模长为1）时，核函数表示两个向量间的余弦距离。\n\n判定某个核函数是否可取有一个充分必要条件(Shawe Taylor and Cristianini, 2004)，核函数可取当且仅当格莱姆/Gram矩阵在输入空间上是半正定的（回忆Gram矩阵 $K_{ij}=k(\\mathbf x_i,\\mathbf x_j)$ ）。核函数表示两个输入向量之间的相似度度量（similarity），实际构造核函数时可在现有核函数的基础上构造新的核函数，有若干准则可保证构造后的新核函数是可取的。\n> 我们已经观察到核函数 $k(\\mathbf x, \\mathbf z)=\\left(\\mathbf x^\\text T\\mathbf z\\right)^2$ 对应的特征向量只包含二次项。此外我们还有，若 $c>0$ ，则核函数 $k(\\mathbf x, \\mathbf z)=\\left(\\mathbf x^\\text T\\mathbf z+c\\right)^2$ 对应的特征向量包含0、1、2次项；此结论可推广至 $k(\\mathbf x,\\mathbf z)=\\left(\\mathbf x^\\text T\\mathbf z+c\\right)^M$ 。\n\n高斯核函数形式为\n\n$$\nk(\\mathbf x, \\mathbf z)=\\exp\\left(-\\frac{||\\mathbf x-\\mathbf z||^2}{2\\sigma^2}\\right),\n$$\n其可取性证明略，这里高斯核函数对应的特征向量维数为无穷。\n\n我们知道在判别问题中判别模型会比生成模型表现更好，但生成模型可处理缺失数据等问题，核方法提供了一种将二者结合的途径。首先使用生成模型得到核函数，再在判别模型中使用该核函数。例如对于生成模型得到的概率密度 $p(\\mathbf x)$，定义如下核函数\n\n$$\nk(\\mathbf x,\\mathbf z)=p(\\mathbf x)p(\\mathbf z),\n$$\n考虑将 $p(\\cdot)$ 作为特征函数，显然该核函数是可取的(valid)。容易说明，以下核函数也是可取的\n\n$$\nk(\\mathbf x,\\mathbf z)=\\sum p(\\mathbf x|i)p(\\mathbf z|i)p(i),\n$$\n此时我们只需将特征函数 $\\phi_j(\\mathbf x)$ 看成是 $\\sqrt{p(j)}p(\\mathbf x|j)$ 即可。\n\n现在假设有长度为 $L$ 的有序观测序列 $\\mathbf X=\\{\\mathbf x_1,...,\\mathbf x_L\\}$ ，隐马尔科夫模型是一种常见的序列生成模型，它将 $p(\\mathbf X)$ 表示为对应有序隐藏状态序列 $\\mathbf Z = \\{\\mathbf z_1,...,\\mathbf z_L\\}$ 的边缘概率。使用核函数表示两个序列之间的相似度\n\n$$\np(\\mathbf X,\\mathbf X^\\prime)=\\sum_\\mathbf Z p(\\mathbf X|\\mathbf Z)p(\\mathbf X^\\prime|\\mathbf Z)p(\\mathbf Z),\n$$\n从而上面两个观测序列 $\\mathbf X$ 和 $\\mathbf X^\\prime$ 均由隐含状态序列 $\\mathbf Z$ 生成。\n\n另一种使用生成模型定义核函数的是Fisher核。使用 $p(\\mathbf x |\\boldsymbol\\theta)$ 表示参数化生成模型，为了衡量两个向量之间的相似度，首先定义Fisher得分(Fisher score)为对数概率的梯度\n\n$$\n\\mathbf g(\\boldsymbol\\theta,\\mathbf x)=\\nabla\\boldsymbol\\theta\\ln p(\\mathbf x|\\mathbf\\theta),\n$$\nFisher核为该Fisher得分的类二次型\n\n$$\nk(\\mathbf x,\\mathbf x^\\prime)=\\mathbf g(\\boldsymbol\\theta, \\mathbf x)^\\text T\\mathbf F^{-1}\\mathbf g(\\boldsymbol\\theta, \\mathbf x^\\prime),\n$$\n其中 $\\mathbf F$ 是 Fisher 信息矩阵\n\n$$\n\\mathbf F=\\mathbb E_\\mathbf x[\\mathbf g(\\boldsymbol\\theta, \\mathbf x)\\mathbf g(\\boldsymbol\\theta, \\mathbf x)^\\text T],\n$$\n这里Fisher矩阵使得Fisher核不受生成模型参数（非线性）变换的影响（例如 $\\boldsymbol\\theta\\rightarrow\\boldsymbol\\psi(\\theta)$ ） 。\n\nS型(sigmoid)高斯核函数形式为\n\n$$\nk(\\mathbf x,\\mathbf x^\\prime)=\\tanh\\left(a\\mathbf x^\\text T\\mathbf x+b\\right),\n$$\n这种形式的核函数对应的Gram矩阵一般来说不满足半正定的条件。\n\n3 径向基函数网络\n---\n---\n\n**1** 推导Nadaraya-Watson模型。假设训练数据为 $\\{\\mathbf x_n,t_n\\}$ ，使用Parzen密度估计对联合分布 $p(\\mathbf x,t)$ 建模\n\n$$\np(\\mathbf x, t)=\\frac{1}{N}\\sum f(\\mathbf x-\\mathbf x_n,t-t_n),\n$$\n其中 $f(\\mathbf x,t)$ 是成分密度函数。使用条件期望作为回归输出\n\n$$\n\\begin{aligned}\ny(\\mathbf x)=&\\mathbb E[t|\\mathbf x]\\\\=&\n\\int tp(t|\\mathbf x)\\text{ d}t\\\\=&\n\\frac{1/N\\int  t\\sum f(\\mathbf x -\\mathbf x_n,t-t_n)\\text{ d}t}{1/N\\int \\sum f(\\mathbf x-\\mathbf x_n,t-t_n)\\text{ d}t}\\\\=&\n\\frac{\\sum \\int tf(\\mathbf x-\\mathbf x_n,t-t_n)\\text{ d}t}{\\sum \\int f(\\mathbf x-\\mathbf x_n,t-t_n)\\text{ d}t},\n\\end{aligned}\n$$\n假设对于任意的 $\\mathbf x$， $t$ 的期望为$0$，从而\n\n$$\n\\int tf(\\mathbf x,t)\\text{ d}t = 0,\n$$\n则\n\n$$\n\\begin{aligned}\ny(\\mathbf x)=&\\frac{\\sum t_n\\int f(\\mathbf x-\\mathbf x_n,t)\\text{ d}t}{\\sum\\int f(\\mathbf x-\\mathbf x_n,t)\\text{ d}t}\\\\=&\n\\frac{\\sum t_n g(\\mathbf x-\\mathbf x_n)}{\\sum g(\\mathbf x-\\mathbf x_n)}\\\\=&\n\\sum t_n k(\\mathbf x,\\mathbf x_n),\n\\end{aligned}\n$$\n其中\n\n$$\nk(\\mathbf x,\\mathbf x_n)=\\frac{g(\\mathbf x-\\mathbf x_n)}{\\sum g(\\mathbf x-\\mathbf x_n)},\n$$\n\n$$\ng(\\mathbf x)=\\int f(\\mathbf x,t)\\text{ d}t,\n$$\n且核函数满足约束\n\n$$\n\\sum k(\\mathbf x,\\mathbf x_n) = 1.\n$$\n\n4 高斯过程\n---\n---\n\n**1** 对于如下的线性回归模型\n\n$$\ny(\\mathbf x)=\\mathbf w^\\text T\\boldsymbol\\phi(\\mathbf x),\n$$\n特征向量由 $M$ 个固定的基函数得到；假设 $\\mathbf w$ 的先验分布为同心高斯型\n\n$$\np(\\mathbf w)=\\mathcal N(\\mathbf w|\\mathbf 0,\\alpha^{-1}\\mathbf I),\n$$\n考察在训练集样本点上函数值的联合分布，向量 $\\mathbf y=(y(\\mathbf x_1),...,y(\\mathbf x_N))^\\text T$ 满足\n\n$$\n\\mathbf y=\\mathbf\\Phi\\mathbf w,\n$$\n其中 $\\mathbf\\Phi$ 是设计矩阵， $\\mathbf\\Phi_{n,\\cdot}=\\boldsymbol\\phi(\\mathbf x_n)^\\text T$ ，从而 $\\mathbf y$ 服从高斯分布，且\n\n$$\n\\mathbb E[\\mathbf y]=\\mathbf\\Phi\\mathbb E[\\mathbf w]=\\mathbf 0,\n$$\n\n$$\n\\text{cov}[\\mathbf y]=\\mathbb E[\\mathbf y\\mathbf y^\\text T]=\\mathbf\\Phi\\mathbb E[\\mathbf w\\mathbf w^\\text T]\\mathbf\\Phi^\\text T=\\frac{1}{\\alpha}\\mathbf\\Phi\\mathbf\\Phi^\\text T=\\mathbf K,\n$$\n这里 $K$ 作为Gram矩阵，其元素为核函数的值\n\n$$\nK_{nm}=k(\\mathbf x_n,\\mathbf x_m)=\\frac{1}{\\alpha}\\boldsymbol\\phi(\\mathbf x_n)^\\text T\\boldsymbol\\phi(\\mathbf x_m).\n$$\n\n**2** 考虑将高斯过程应用到回归问题，考虑有噪的观测目标变量\n\n$$\nt_n=y_n+\\epsilon_n,\n$$\n这里考虑服从高斯分布的噪声过程\n\n$$\np(t_n|y_n)=\\mathcal N(t_n|y_n, \\beta^{-1}),\n$$\n假设每个样本点上的噪声是独立的，从而\n\n$$\np(\\mathbf t|\\mathbf y)=\\mathcal N(\\mathbf t|\\mathbf y,\\beta^{-1}\\mathbf I_N),\n$$\n由高斯过程的定义，$\\mathbf y$ 的边缘分布满足\n\n$$\np(\\mathbf y)=\\mathcal N(\\mathbf y|\\mathbf 0, \\mathbf K),\n$$\n积分消去 $\\mathbf y$ ，得到 $\\mathbf t$ 的边缘分布\n\n$$\np(\\mathbf t) = \\int \\text{ d}\\mathbf yp(\\mathbf{t|y})p(\\mathbf y)=\\mathcal N(\\mathbf t|\\mathbf 0, \\mathbf K+\\beta^{-1}\\mathbf I_N)=\\mathcal N(\\mathbf t|\\mathbf0, \\mathbf C).\n$$\n由于来自 $y(\\mathbf x)$ 和 $\\epsilon$ 的两个高斯噪声相独立，因此协方差可直接相加。\n一种常用的高斯过程回归核函数具有如下形式\n\n$$\nk(\\mathbf x_n,\\mathbf x_m)=\\theta_0\\exp\\left\\{-\\frac{\\theta_1} 2||\\mathbf x_n-\\mathbf x_m||^2\\right\\}+\\theta_2+\\theta_3\\mathbf x_n^\\text T\\mathbf x_m.\n$$\n对于新输入 $\\mathbf x_{N+1}$ ，高斯过程回归模型给出预测条件分布 $p(t_{N+1}|\\mathbf t_N;\\mathbf x_1,...\\mathbf x_N,\\mathbf x_{N+1})$ ，由于  \n\n$$\np(\\mathbf t_{N+1})=\\mathcal N(\\mathbf t_{N+1}|\\mathbf 0, \\mathbf C_{N+1}),\n$$\n分解该协方差矩阵\n\n$$\n\\mathbf C_{N+1} =\\left(\n\\begin{matrix}\n    \\mathbf C_N       &\\mathbf k\\\\\n    \\mathbf k^\\text T & c\n\\end{matrix}\\right)\n$$\n这里 $\\mathbf k$ 由 $k(\\mathbf x_n,\\mathbf x_{N+1})$ 组成，且 $c=k(\\mathbf x_{N+1},\\mathbf x_{N+1})+\\beta^{-1},$ 应用在前序章节中的结论可知 $p(t_{N+1}|\\mathbf t)$ 服从高斯分布，且其数值特征为\n\n$$\nm(\\mathbf x_{N+1})=\\mathbf k^\\text T\\mathbf C_N^{-1}\\mathbf t,\n$$\n\n$$\n\\sigma^2(\\mathbf x_{N+1})=c-\\mathbf k^\\text T\\mathbf C_N^{-1}\\mathbf k,\n$$","author":"pandoxone"}