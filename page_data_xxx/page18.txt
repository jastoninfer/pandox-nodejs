
核方法
===
---

1 对偶表示
---
---
**1** 我们从线性参数化模型推导演绎给出核函数的形式。考虑带L2正则项（参数平方误差项）的线性回归模型，误差函数由下式给出

$$
 J(\mathbf w)=\frac{1}{2}\sum\left(\mathbf w^\text T\boldsymbol\phi(\mathbf x_n)-t_n\right)^2+\frac{\lambda}{2}\mathbf w^\text T\mathbf w, 
$$
该误差函数对 $\mathbf w$ 的梯度为

$$
\nabla_\mathbf w J(\mathbf w)=\lambda\mathbf w+\sum \boldsymbol\phi(\mathbf x_n)\left(\mathbf w^\text T\boldsymbol\phi(\mathbf x_n)-t_n\right),
$$
令 $\nabla_\mathbf w=0$ ，得

$$
\mathbf w=\sum a_n\boldsymbol\phi(\mathbf x_n)=\mathbf\Phi^\text T\mathbf a,
$$
其中

$$
\mathbf\Phi^\text T=(\boldsymbol\phi(\mathbf x_1),...,\boldsymbol\phi(\mathbf x_n)),
$$

$$
a_n=-\frac 1\lambda\left(\mathbf w^\text T\boldsymbol\phi(\mathbf x_n)-t_n)\right),
$$
现在将误差项表示为 $\mathbf a$ 的函数，将 $\mathbf w=\mathbf\Phi^\text T\mathbf a$ 回带误差函数，得到

$$
\begin{aligned}
J(\mathbf a)=&\frac 1 2\sum \left(\mathbf a^\text T\mathbf\Phi\boldsymbol\phi(\mathbf x_n)-t_n\right)^2+\frac\lambda 2\mathbf a^\text T\mathbf\Phi\mathbf\Phi^\text T\mathbf a\\=&
\frac\lambda 2\mathbf a^\text T\mathbf\Phi\mathbf\Phi^\text T\mathbf a+\frac 1 2(\mathbf a^\text T\mathbf\Phi\mathbf\Phi^\text T-\mathbf t^\text T)(\mathbf\Phi\mathbf\Phi^\text T\mathbf a-\mathbf t)\\=&
\frac\lambda 2\mathbf a^\text T\mathbf\Phi\mathbf\Phi^\text T\mathbf a+\frac 1 2\left(\mathbf a^\text T\mathbf\Phi\mathbf\Phi^\text T\mathbf\Phi\mathbf\Phi^\text T\mathbf a-\mathbf a^\text T\mathbf\Phi\mathbf\Phi^\text T\mathbf t-\mathbf t^\text T\mathbf\Phi\mathbf\Phi^\text T\mathbf a+\mathbf t^\text T\mathbf t\right),
\end{aligned}
$$
现在记格莱姆/Gram矩阵为 $\mathbf K=\mathbf\Phi\mathbf\Phi^\text T$ ，从而 $K_{ij}=\boldsymbol\phi(\mathbf x_i)^\text T\boldsymbol\phi(\mathbf x_j)=k(\mathbf x_i,\mathbf x_j)=K_{ji}$ ，重写误差函数为

$$
J(\mathbf a)=\frac\lambda 2\mathbf a^\text T\mathbf K\mathbf a+\frac 1 2\left(\mathbf a^\text T\mathbf K\mathbf K\mathbf a-2\mathbf a^\text T\mathbf K\mathbf t+\mathbf t^\text T\mathbf t\right),
$$
令其关于 $\mathbf a$ 的梯度为零

$$
\nabla_\mathbf aJ(\mathbf a)=\lambda\mathbf K\mathbf a+\mathbf K\mathbf K\mathbf a-\mathbf K\mathbf t=0,
$$
得到

$$
\mathbf a=\left(\mathbf K+\lambda\mathbf I\right)^{-1}\mathbf t,
$$
由于模型输出

$$
y(\mathbf w)=\mathbf w^\text T\boldsymbol\phi(\mathbf x),
$$
用 $\mathbf a$ 替换 $\mathbf w$ 并利用上述结论，有

$$
\begin{aligned}
y(\mathbf a)=&\mathbf a^\text T\mathbf\Phi\boldsymbol\phi(\mathbf x)\\=&
\boldsymbol\phi(\mathbf x)^\text T\mathbf\Phi^\text T\mathbf a\\=&
\boldsymbol\phi(\mathbf x)^\text T\left(\boldsymbol\phi(\mathbf x_1),...,\boldsymbol\phi(\mathbf x_n)\right)\left(\mathbf K+\lambda\mathbf I\right)^{-1}\mathbf t\\=&
\mathbf k(\mathbf x)^\text T\left(\mathbf K+\lambda\mathbf I\right)^{-1}\mathbf t.
\end{aligned}
$$

需要注意的是，为了求 $\mathbf a$，我们需要计算一个 $N\times N$ 矩阵的逆， $N$ 是样本个数；而如果直接求 $\mathbf w$ ,需要在特征空间中对一个 $M\times M$ 的矩阵求逆，这里 $M$ 是特征空间维数；一般来说 $M\ll N$ ,因此看起来计算量反而增大了，但是新的模型输入完全依赖于核函数 $k(\mathbf x, \mathbf x^\prime)$ (除输入数据本身外)，从而可以跳过特征向量 $\boldsymbol\phi(\mathbf x)$ ，不受特征维数的制约。

2 构造核函数
---
---

**1** 核函数有两种构造方法，其一是使用特征函数 $\mathbf\phi(\cdot)$ 函数间接表示核函数 $k(\cdot)$ ，这里 $k(\mathbf x,\mathbf x^\prime)=\boldsymbol\phi(\mathbf x)^\text T\boldsymbol\phi(\mathbf x^\prime)$ ；另一种途径是直接构造核函数，但是核函数的选取不是随意的，它需要隐式决定某个特征函数。例如考虑核函数

$$
k(\mathbf x,\mathbf z)=\left(\mathbf x^\text T\mathbf z\right)^2,
$$
简便起见，考虑输入空间维数为2的情况，即 $\mathbf x = (x_1,x_2)^\text T$ ，从而

$$
\begin{aligned}
k(\mathbf x, \mathbf z) =&\left(\mathbf x^\text T\mathbf z\right)^2\\=&
\left(x_1z_1+x_2z_2\right)^2\\=&
x_1^2z_1^2+x_2^2z_2^2+2x_1x_2z_1z_2\\=&
x_1^2\cdot z_1^2+x_2^2\cdot z_2^2+\sqrt 2x_1x_2\cdot \sqrt 2z_1z_2\\=&
\boldsymbol\phi(x)^\text T\boldsymbol\phi(z),
\end{aligned}
$$
其中 $\boldsymbol\phi(\mathbf x)=\left(x_1^2, x_2^2,\sqrt 2x_2x_2\right)^\text T$ ，显然特征向量包含原输入向量的所有二次项。
> 当两个输入向量相等时，核函数表示该向量在特征空间中的规范二次型。
> 当特征向量是规范的（模长为1）时，核函数表示两个向量间的余弦距离。

判定某个核函数是否可取有一个充分必要条件(Shawe Taylor and Cristianini, 2004)，核函数可取当且仅当格莱姆/Gram矩阵在输入空间上是半正定的（回忆Gram矩阵 $K_{ij}=k(\mathbf x_i,\mathbf x_j)$ ）。核函数表示两个输入向量之间的相似度度量（similarity），实际构造核函数时可在现有核函数的基础上构造新的核函数，有若干准则可保证构造后的新核函数是可取的。
> 我们已经观察到核函数 $k(\mathbf x, \mathbf z)=\left(\mathbf x^\text T\mathbf z\right)^2$ 对应的特征向量只包含二次项。此外我们还有，若 $c>0$ ，则核函数 $k(\mathbf x, \mathbf z)=\left(\mathbf x^\text T\mathbf z+c\right)^2$ 对应的特征向量包含0、1、2次项；此结论可推广至 $k(\mathbf x,\mathbf z)=\left(\mathbf x^\text T\mathbf z+c\right)^M$ 。

高斯核函数形式为

$$
k(\mathbf x, \mathbf z)=\exp\left(-\frac{||\mathbf x-\mathbf z||^2}{2\sigma^2}\right),
$$
其可取性证明略，这里高斯核函数对应的特征向量维数为无穷。

我们知道在判别问题中判别模型会比生成模型表现更好，但生成模型可处理缺失数据等问题，核方法提供了一种将二者结合的途径。首先使用生成模型得到核函数，再在判别模型中使用该核函数。例如对于生成模型得到的概率密度 $p(\mathbf x)$，定义如下核函数

$$
k(\mathbf x,\mathbf z)=p(\mathbf x)p(\mathbf z),
$$
考虑将 $p(\cdot)$ 作为特征函数，显然该核函数是可取的(valid)。容易说明，以下核函数也是可取的

$$
k(\mathbf x,\mathbf z)=\sum p(\mathbf x|i)p(\mathbf z|i)p(i),
$$
此时我们只需将特征函数 $\phi_j(\mathbf x)$ 看成是 $\sqrt{p(j)}p(\mathbf x|j)$ 即可。

现在假设有长度为 $L$ 的有序观测序列 $\mathbf X=\{\mathbf x_1,...,\mathbf x_L\}$ ，隐马尔科夫模型是一种常见的序列生成模型，它将 $p(\mathbf X)$ 表示为对应有序隐藏状态序列 $\mathbf Z = \{\mathbf z_1,...,\mathbf z_L\}$ 的边缘概率。使用核函数表示两个序列之间的相似度

$$
p(\mathbf X,\mathbf X^\prime)=\sum_\mathbf Z p(\mathbf X|\mathbf Z)p(\mathbf X^\prime|\mathbf Z)p(\mathbf Z),
$$
从而上面两个观测序列 $\mathbf X$ 和 $\mathbf X^\prime$ 均由隐含状态序列 $\mathbf Z$ 生成。

另一种使用生成模型定义核函数的是Fisher核。使用 $p(\mathbf x |\boldsymbol\theta)$ 表示参数化生成模型，为了衡量两个向量之间的相似度，首先定义Fisher得分(Fisher score)为对数概率的梯度

$$
\mathbf g(\boldsymbol\theta,\mathbf x)=\nabla\boldsymbol\theta\ln p(\mathbf x|\mathbf\theta),
$$
Fisher核为该Fisher得分的类二次型

$$
k(\mathbf x,\mathbf x^\prime)=\mathbf g(\boldsymbol\theta, \mathbf x)^\text T\mathbf F^{-1}\mathbf g(\boldsymbol\theta, \mathbf x^\prime),
$$
其中 $\mathbf F$ 是 Fisher 信息矩阵

$$
\mathbf F=\mathbb E_\mathbf x[\mathbf g(\boldsymbol\theta, \mathbf x)\mathbf g(\boldsymbol\theta, \mathbf x)^\text T],
$$
这里Fisher矩阵使得Fisher核不受生成模型参数（非线性）变换的影响（例如 $\boldsymbol\theta\rightarrow\boldsymbol\psi(\theta)$ ） 。

S型(sigmoid)高斯核函数形式为

$$
k(\mathbf x,\mathbf x^\prime)=\tanh\left(a\mathbf x^\text T\mathbf x+b\right),
$$
这种形式的核函数对应的Gram矩阵一般来说不满足半正定的条件。

3 径向基函数网络
---
---

**1** 推导Nadaraya-Watson模型。假设训练数据为 $\{\mathbf x_n,t_n\}$ ，使用Parzen密度估计对联合分布 $p(\mathbf x,t)$ 建模

$$
p(\mathbf x, t)=\frac{1}{N}\sum f(\mathbf x-\mathbf x_n,t-t_n),
$$
其中 $f(\mathbf x,t)$ 是成分密度函数。使用条件期望作为回归输出

$$
\begin{aligned}
y(\mathbf x)=&\mathbb E[t|\mathbf x]\\=&
\int tp(t|\mathbf x)\text{ d}t\\=&
\frac{1/N\int  t\sum f(\mathbf x -\mathbf x_n,t-t_n)\text{ d}t}{1/N\int \sum f(\mathbf x-\mathbf x_n,t-t_n)\text{ d}t}\\=&
\frac{\sum \int tf(\mathbf x-\mathbf x_n,t-t_n)\text{ d}t}{\sum \int f(\mathbf x-\mathbf x_n,t-t_n)\text{ d}t},
\end{aligned}
$$
假设对于任意的 $\mathbf x$， $t$ 的期望为$0$，从而

$$
\int tf(\mathbf x,t)\text{ d}t = 0,
$$
则

$$
\begin{aligned}
y(\mathbf x)=&\frac{\sum t_n\int f(\mathbf x-\mathbf x_n,t)\text{ d}t}{\sum\int f(\mathbf x-\mathbf x_n,t)\text{ d}t}\\=&
\frac{\sum t_n g(\mathbf x-\mathbf x_n)}{\sum g(\mathbf x-\mathbf x_n)}\\=&
\sum t_n k(\mathbf x,\mathbf x_n),
\end{aligned}
$$
其中

$$
k(\mathbf x,\mathbf x_n)=\frac{g(\mathbf x-\mathbf x_n)}{\sum g(\mathbf x-\mathbf x_n)},
$$

$$
g(\mathbf x)=\int f(\mathbf x,t)\text{ d}t,
$$
且核函数满足约束

$$
\sum k(\mathbf x,\mathbf x_n) = 1.
$$

4 高斯过程
---
---

**1** 对于如下的线性回归模型

$$
y(\mathbf x)=\mathbf w^\text T\boldsymbol\phi(\mathbf x),
$$
特征向量由 $M$ 个固定的基函数得到；假设 $\mathbf w$ 的先验分布为同心高斯型

$$
p(\mathbf w)=\mathcal N(\mathbf w|\mathbf 0,\alpha^{-1}\mathbf I),
$$
考察在训练集样本点上函数值的联合分布，向量 $\mathbf y=(y(\mathbf x_1),...,y(\mathbf x_N))^\text T$ 满足

$$
\mathbf y=\mathbf\Phi\mathbf w,
$$
其中 $\mathbf\Phi$ 是设计矩阵， $\mathbf\Phi_{n,\cdot}=\boldsymbol\phi(\mathbf x_n)^\text T$ ，从而 $\mathbf y$ 服从高斯分布，且

$$
\mathbb E[\mathbf y]=\mathbf\Phi\mathbb E[\mathbf w]=\mathbf 0,
$$

$$
\text{cov}[\mathbf y]=\mathbb E[\mathbf y\mathbf y^\text T]=\mathbf\Phi\mathbb E[\mathbf w\mathbf w^\text T]\mathbf\Phi^\text T=\frac{1}{\alpha}\mathbf\Phi\mathbf\Phi^\text T=\mathbf K,
$$
这里 $K$ 作为Gram矩阵，其元素为核函数的值

$$
K_{nm}=k(\mathbf x_n,\mathbf x_m)=\frac{1}{\alpha}\boldsymbol\phi(\mathbf x_n)^\text T\boldsymbol\phi(\mathbf x_m).
$$

**2** 考虑将高斯过程应用到回归问题，考虑有噪的观测目标变量

$$
t_n=y_n+\epsilon_n,
$$
这里考虑服从高斯分布的噪声过程

$$
p(t_n|y_n)=\mathcal N(t_n|y_n, \beta^{-1}),
$$
假设每个样本点上的噪声是独立的，从而

$$
p(\mathbf t|\mathbf y)=\mathcal N(\mathbf t|\mathbf y,\beta^{-1}\mathbf I_N),
$$
由高斯过程的定义，$\mathbf y$ 的边缘分布满足

$$
p(\mathbf y)=\mathcal N(\mathbf y|\mathbf 0, \mathbf K),
$$
积分消去 $\mathbf y$ ，得到 $\mathbf t$ 的边缘分布

$$
p(\mathbf t) = \int \text{ d}\mathbf yp(\mathbf{t|y})p(\mathbf y)=\mathcal N(\mathbf t|\mathbf 0, \mathbf K+\beta^{-1}\mathbf I_N)=\mathcal N(\mathbf t|\mathbf0, \mathbf C).
$$
由于来自 $y(\mathbf x)$ 和 $\epsilon$ 的两个高斯噪声相独立，因此协方差可直接相加。
一种常用的高斯过程回归核函数具有如下形式

$$
k(\mathbf x_n,\mathbf x_m)=\theta_0\exp\left\{-\frac{\theta_1} 2||\mathbf x_n-\mathbf x_m||^2\right\}+\theta_2+\theta_3\mathbf x_n^\text T\mathbf x_m.
$$
对于新输入 $\mathbf x_{N+1}$ ，高斯过程回归模型给出预测条件分布 $p(t_{N+1}|\mathbf t_N;\mathbf x_1,...\mathbf x_N,\mathbf x_{N+1})$ ，由于  

$$
p(\mathbf t_{N+1})=\mathcal N(\mathbf t_{N+1}|\mathbf 0, \mathbf C_{N+1}),
$$
分解该协方差矩阵

$$
\mathbf C_{N+1} =\left(
\begin{matrix}
    \mathbf C_N       &\mathbf k\\
    \mathbf k^\text T & c
\end{matrix}\right)
$$
这里 $\mathbf k$ 由 $k(\mathbf x_n,\mathbf x_{N+1})$ 组成，且 $c=k(\mathbf x_{N+1},\mathbf x_{N+1})+\beta^{-1},$ 应用在前序章节中的结论可知 $p(t_{N+1}|\mathbf t)$ 服从高斯分布，且其数值特征为

$$
m(\mathbf x_{N+1})=\mathbf k^\text T\mathbf C_N^{-1}\mathbf t,
$$

$$
\sigma^2(\mathbf x_{N+1})=c-\mathbf k^\text T\mathbf C_N^{-1}\mathbf k,
$$